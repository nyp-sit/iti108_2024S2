{"cells":[{"cell_type":"markdown","metadata":{"id":"VtXdc1O6Gm-a"},"source":["# Token Classification (Named Entity Recognition)\n","\n","In this practical we will learn how to use the HuggingFace Transformers library to perform token classification.\n","\n","Just like what we did in previous practical on BERT Transformer, we will use the DistilBERT transformer to classify each and every word (token) in a sentence.\n"]},{"cell_type":"markdown","metadata":{"id":"XJy827DuG3b7"},"source":["## Install Transformers\n","\n","Run the following cell to install the HuggingFace Transformers library."]},{"cell_type":"code","source":["!pip install transformers datasets==1.2.1"],"metadata":{"id":"sUb65UVgLepB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727278408732,"user_tz":-480,"elapsed":3103,"user":{"displayName":"NYP weech","userId":"03211871160483530494"}},"outputId":"7a3914e8-93de-4fdf-ce0b-d39f970afb69"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n","Requirement already satisfied: datasets==1.2.1 in /usr/local/lib/python3.10/dist-packages (1.2.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==1.2.1) (1.26.4)\n","Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from datasets==1.2.1) (14.0.2)\n","Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from datasets==1.2.1) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==1.2.1) (2.1.4)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==1.2.1) (2.32.3)\n","Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.10/dist-packages (from datasets==1.2.1) (4.49.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==1.2.1) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==1.2.1) (0.70.16)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.2.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.2.1) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.2.1) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.2.1) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==1.2.1) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==1.2.1) (2024.2)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==1.2.1) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==1.2.1) (1.16.0)\n"]}]},{"cell_type":"markdown","metadata":{"id":"apSbg63-PhYf"},"source":["## Get the data\n","\n","In this lab, we will use the CoNLL-2003 dataset."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"y0wg6EKWOMhj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727278744407,"user_tz":-480,"elapsed":330350,"user":{"displayName":"NYP weech","userId":"03211871160483530494"}},"outputId":"36a77063-ec8b-4f84-e6c4-fb6ad3f83b12"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-09-25 15:33:34--  https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/datasets/conll2003.zip\n","Resolving nyp-aicourse.s3.ap-southeast-1.amazonaws.com (nyp-aicourse.s3.ap-southeast-1.amazonaws.com)... 52.219.132.219, 3.5.150.171, 52.219.37.7, ...\n","Connecting to nyp-aicourse.s3.ap-southeast-1.amazonaws.com (nyp-aicourse.s3.ap-southeast-1.amazonaws.com)|52.219.132.219|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 795342 (777K) [application/zip]\n","Saving to: ‘conll2003.zip.1’\n","\n","conll2003.zip.1     100%[===================>] 776.70K   938KB/s    in 0.8s    \n","\n","2024-09-25 15:33:36 (938 KB/s) - ‘conll2003.zip.1’ saved [795342/795342]\n","\n","Archive:  conll2003.zip\n","replace token_test.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n","replace token_train.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"]}],"source":["!wget https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/datasets/conll2003.zip\n","!unzip conll2003.zip"]},{"cell_type":"markdown","source":["## Process the data\n","\n","The data file is in CoNLL format:\n","\n","```\n","sentence1-word1  PPTag-1-1  ChunkTag-1-1  NERTag-1-1\n","sentence1-word2  PPTag-1-2  ChunkTag-1-2  NERTag-1-2\n","sentence1-word3  PPTag-1-3  ChunkTag-1-3  NERTag-1-3\n","<empty line>\n","sentence2-word1  PPTag-2-1  ChunkTag-2-1  NERTag-2-1\n","sentence2-word2  PPTag-2-2  ChunkTag-2-2  NERTag-2-2\n","...\n","sentence2-wordn  PPTag-2-n  ChunkTag-2-n  NERTag-2-n\n","<empty line>\n","...\n","```\n","\n","For example, the sentence \"U.N. official Ekeus heads for Baghdad.\" will be represented as follow in CoNLL format:\n","\n","```\n","U.N.      NNP  I-NP  I-ORG\n","official  NN   I-NP  O\n","Ekeus     NNP  I-NP  I-PER\n","heads     VBZ  I-VP  O\n","for       IN   I-PP  O\n","Baghdad   NNP  I-NP  I-LOC\n",".         .    O     O\n","```"],"metadata":{"id":"BgLdC0e1MjqQ"}},{"cell_type":"markdown","source":["We define a function to read the data file line by line and combined lines that belong to a sentence into a list of words and list of tags.\n","\n","As we are only interested in the Named Entity Recognition (NER) tags, we will only extract tags from column_index 3."],"metadata":{"id":"jLL7uGOsMlAf"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"e7mWDJnwTy9S","executionInfo":{"status":"ok","timestamp":1727278744407,"user_tz":-480,"elapsed":7,"user":{"displayName":"NYP weech","userId":"03211871160483530494"}}},"outputs":[],"source":["# This function returns a 2D list of words and a 2D list of labels\n","# corresponding to each word.\n","\n","def load_conll(filepath, delimiter=' ', word_column_index=0, label_column_index=3):\n","    all_texts = []\n","    all_tags = []\n","\n","    texts = []\n","    tags = []\n","\n","    # Opens the file.\n","    #\n","    with open(filepath, \"r\") as f:\n","\n","        # Loops through each line\n","        for line in f:\n","\n","            # Split each line by its delimiter (default is a space)\n","            tokens = line.split(delimiter)\n","\n","            # If the line is empty, treat it as the end of the\n","            # previous sentence, and construct a new sentence\n","            #\n","            if len(tokens) == 1:\n","                # Append the sentence\n","                #\n","                all_texts.append(texts)\n","                all_tags.append(tags)\n","\n","                # Create a new sentence\n","                #\n","                texts = []\n","                tags = []\n","            else:\n","                # Not yet end of the sentence, continue to add\n","                # words into the current sentence\n","                #\n","                thistext = tokens[word_column_index].replace('\\n', '')\n","                thistag = tokens[label_column_index].replace('\\n', '')\n","\n","                texts.append(thistext)\n","                tags.append(thistag)\n","\n","    # Insert the last sentence if it contains at least 1 word.\n","    #\n","    if len(texts) > 0:\n","        all_texts.append(texts)\n","        all_tags.append(tags)\n","\n","    # Return the result to the caller\n","    #\n","    return all_texts, all_tags\n"]},{"cell_type":"markdown","metadata":{"id":"apjtIJjOWWrs"},"source":["We will now process our files with the function and examine the outputs."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"DgARQPbyWdDh","executionInfo":{"status":"ok","timestamp":1727278744407,"user_tz":-480,"elapsed":6,"user":{"displayName":"NYP weech","userId":"03211871160483530494"}}},"outputs":[],"source":["train_texts, train_tags = load_conll(\"token_train.txt\")\n","val_texts, val_tags = load_conll(\"token_test.txt\")"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"P7O43-HxWlmn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727278744407,"user_tz":-480,"elapsed":6,"user":{"displayName":"NYP weech","userId":"03211871160483530494"}},"outputId":"02ef1392-7037-4584-a692-9312f88e5fc3"},"outputs":[{"output_type":"stream","name":"stdout","text":["[['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], ['Peter', 'Blackburn'], ['BRUSSELS', '1996-08-22']]\n","[['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O'], ['B-PER', 'I-PER'], ['B-LOC', 'O']]\n"]}],"source":["print(train_texts[:3])\n","print(train_tags[:3])"]},{"cell_type":"markdown","metadata":{"id":"dVm7WR1kXm2F"},"source":["## Tokenization\n","\n","Now we have our texts and labels. Before we can feed the texts and labels into our model for training, we need to tokenize our texts and also encode our labels into numeric forms.\n","\n","We first define the token labels we need and define the mapping to a numeric index.\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"6qOghzNYqE5v","executionInfo":{"status":"ok","timestamp":1727278744407,"user_tz":-480,"elapsed":5,"user":{"displayName":"NYP weech","userId":"03211871160483530494"}}},"outputs":[],"source":["# Define a list of unique token labels that we will recognize\n","#\n","label_names = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n","\n","# Define a dictionary to map txt label to numeric label\n","label2id = {label:i for i, label in enumerate(label_names)}"]},{"cell_type":"markdown","source":["We will also import the tokenizer."],"metadata":{"id":"ln2Jj3J8UNXi"}},{"cell_type":"code","execution_count":9,"metadata":{"id":"cT5Wu4sFYorz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727278748298,"user_tz":-480,"elapsed":3895,"user":{"displayName":"NYP weech","userId":"03211871160483530494"}},"outputId":"6ed2b1d5-67d7-4128-dede-4484791faf0f"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}],"source":["from transformers import AutoTokenizer\n","\n","model_checkpoint = 'distilbert-base-uncased'\n","# Initialize the DistilBERT tokenizer.\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, is_fast=True)\n"]},{"cell_type":"markdown","source":["Before we tokenize our texts,  let's look at a potential problem that can happen we do tokenization. Transformers model like Bert or DistilBert uses WordPiece tokenization, meaning that a single word can sometimes be split into multiple tokens (this is done to solve the out-of-vocabulary problem for rare words). For example, DistilBert’s tokenizer would split the `[\"Nadim\", \"Ladki\"]` into the tokens `[[CLS], \"na\", \"##im\",\"lad\", ##ki\", [SEP]]`. This is a problem for us because we have exactly one tag per token in the original dataset. If the tokenizer splits a token into multiple sub-tokens, then we will end up with a mismatch between our tokens and our labels, as illustrated below:\n","\n","Before tokenization with WordPiece, it is one to one matching between tokens and tags:\n","\n","```\n","tokens = [\"Nadim\", \"Ladki\"]\n","labels = ['B-PER', 'I-PER']\n","```\n","\n","After tokenization with WordPiece, there is no more one-to-one match between them:\n","```\n","tokens = [\"[CLS]\", \"nad\", \"##im\", \"lad\", \"##ki\", \"[SEP]\"]\n","labels = ['B-PER', 'I-PER']\n","```\n","\n","One way to handle this is to only train on the tag labels for the first subtoken of a split token. We can do this in Transformers by setting the labels we wish to ignore to -100. We will also ignore special tokens like `[CLS]` and `[SEP]`. In the example above, if the label for 'Nadim' is 1 (index for B-PER) and 'Ladki' is 2 (index for I-PER), we would set the labels as follows:\n","\n","```\n","tokens = [\"[CLS]\", \"nad\", \"##im\", \"lad\", \"##ki\", \"[SEP]\"]\n","labels = [-100, 1, -100, 2, -100, -100]\n","```\n","\n","But how do we know which are sub-tokens and the special tokens to ignore? Fortunately, the Huggingface tokenize provides us a way to do it: `word_ids`. `word_ids` will tell us which word each token comes from, as well as which words are special tokens (e.g. `[CLS]`).\n","\n","For example, the `word_ids` for the following tokens will be:\n","\n","```\n","tokens = [\"[CLS]\", \"nad\", \"##im\", \"lad\", \"##ki\", \"[SEP]\"]\n","word_ids = [None, 0, 0, 1, 1, None]\n","```\n","\n","`None` means it is a special token. You can see that `\"nad\"`, `\"##im\"` are both having word_ids `0`, which means both tokens comes from the word at index 0, i.e. `\"nadim\"`. Similarly, `\"lad\"` and `\"##ki\"` have word_ids of `1`, which means both comes from the 2nd word, i.e. word at index 1.\n","\n","\n","So we can simply use the following logic to decide how to label each of the processed tokens (i.e tokens that have already processed by the tokenizer, and consist of special tokens and subtokens):\n","- if a token has a `word_id` of `None`, we will set its corresponding label to `-100`. - if the `word_id` of the token appears the 1st time, i.e. different from previous `word_id`, set the label of the token to the corresponding original label.\n","- if the `word_id` is the same as previous `word_id`, set the label for the tokens to `-100`\n","\n","The following function `tokenize_and_align_labels()` takes in original tags and encode it according to the logic described above.\n","\n","Note that for this version of HuggingFace, we need to supply the label as part of the dictionary. That is why we create additional entry `tokenized_inputs['labels']` to hold the labels."],"metadata":{"id":"pd9-dD_dPBaH"}},{"cell_type":"code","execution_count":10,"metadata":{"id":"2VPX7Gl5LAcf","executionInfo":{"status":"ok","timestamp":1727278748298,"user_tz":-480,"elapsed":2,"user":{"displayName":"NYP weech","userId":"03211871160483530494"}}},"outputs":[],"source":["# set the max sequence length and require padding\n","max_length=128\n","padding=True\n","\n","def tokenize_and_align_labels(texts, all_tags):\n","\n","    tokenized_inputs = tokenizer(\n","        texts,\n","        max_length=max_length,\n","        padding=padding,\n","        truncation=True,\n","        is_split_into_words=True,\n","    )\n","\n","    labels = []\n","\n","    for i, tags in enumerate(all_tags):\n","        word_ids = tokenized_inputs[i].word_ids\n","        tokens = tokenized_inputs[i].ids\n","        previous_word_idx = None\n","        label_ids = []\n","\n","        for word_idx in word_ids:\n","            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n","            # ignored in the loss function.\n","            if word_idx is None:\n","                label_ids.append(-100)\n","            # We set the label for the first token of each word.\n","            elif word_idx != previous_word_idx:\n","                label_ids.append(int(label2id[tags[word_idx]]))\n","            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n","            # the label_all_tokens flag.\n","            else:\n","                label_ids.append(-100)\n","\n","            previous_word_idx = word_idx\n","\n","        labels.append(label_ids)\n","\n","        tokenized_inputs['labels'] = labels\n","\n","    return tokenized_inputs, labels"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"WoQ1PY2ZLAcf","executionInfo":{"status":"ok","timestamp":1727278750353,"user_tz":-480,"elapsed":2057,"user":{"displayName":"NYP weech","userId":"03211871160483530494"}}},"outputs":[],"source":["train_encodings, train_labels = tokenize_and_align_labels(train_texts, train_tags)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"pUoyXw3TLAcg","executionInfo":{"status":"ok","timestamp":1727278750937,"user_tz":-480,"elapsed":586,"user":{"displayName":"NYP weech","userId":"03211871160483530494"}}},"outputs":[],"source":["val_encodings, val_labels = tokenize_and_align_labels(val_texts, val_tags)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"FpwVcZEGpnv9","executionInfo":{"status":"ok","timestamp":1727278769644,"user_tz":-480,"elapsed":18708,"user":{"displayName":"NYP weech","userId":"03211871160483530494"}}},"outputs":[],"source":["import tensorflow as tf\n","\n","batch_size = 16\n","\n","train_dataset = tf.data.Dataset.from_tensor_slices((\n","    dict(train_encodings),\n","    train_labels\n",")).batch(batch_size)\n","\n","val_dataset = tf.data.Dataset.from_tensor_slices((\n","    dict(val_encodings),\n","    val_labels\n",")).batch(batch_size)"]},{"cell_type":"code","source":["print(train_encodings[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PnuUixUBHcCm","executionInfo":{"status":"ok","timestamp":1727278769644,"user_tz":-480,"elapsed":14,"user":{"displayName":"NYP weech","userId":"03211871160483530494"}},"outputId":"2ace8985-a100-4c9e-c801-1c5525c211ac"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"]}]},{"cell_type":"markdown","metadata":{"id":"zxisdqSj36C-"},"source":["Run the following cell below to see the first few samples of the train dataset to see if they looks all right."]},{"cell_type":"code","source":["iterator = iter(train_dataset)\n","\n","for i in range(1):\n","    print (train_texts[i])\n","    print(iterator.get_next())\n","    print (\"---\")"],"metadata":{"id":"xAUJa6Y4WlMv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727278769644,"user_tz":-480,"elapsed":14,"user":{"displayName":"NYP weech","userId":"03211871160483530494"}},"outputId":"49405732-5c07-49ab-efa6-a325d9c3957f"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n","({'input_ids': <tf.Tensor: shape=(16, 128), dtype=int32, numpy=\n","array([[  101,  7327, 19164, ...,     0,     0,     0],\n","       [  101,  2848, 13934, ...,     0,     0,     0],\n","       [  101,  9371,  2727, ...,     0,     0,     0],\n","       ...,\n","       [  101,  1996,  7327, ...,     0,     0,     0],\n","       [  101,  8351,  2031, ...,     0,     0,     0],\n","       [  101,  2329,  6617, ...,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(16, 128), dtype=int32, numpy=\n","array([[1, 1, 1, ..., 0, 0, 0],\n","       [1, 1, 1, ..., 0, 0, 0],\n","       [1, 1, 1, ..., 0, 0, 0],\n","       ...,\n","       [1, 1, 1, ..., 0, 0, 0],\n","       [1, 1, 1, ..., 0, 0, 0],\n","       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>, 'labels': <tf.Tensor: shape=(16, 128), dtype=int32, numpy=\n","array([[-100,    3,    0, ..., -100, -100, -100],\n","       [-100,    1,    2, ..., -100, -100, -100],\n","       [-100,    5,    0, ..., -100, -100, -100],\n","       ...,\n","       [-100,    0,    3, ..., -100, -100, -100],\n","       [-100,    0,    0, ..., -100, -100, -100],\n","       [-100,    7,    0, ..., -100, -100, -100]], dtype=int32)>}, <tf.Tensor: shape=(16, 128), dtype=int32, numpy=\n","array([[-100,    3,    0, ..., -100, -100, -100],\n","       [-100,    1,    2, ..., -100, -100, -100],\n","       [-100,    5,    0, ..., -100, -100, -100],\n","       ...,\n","       [-100,    0,    3, ..., -100, -100, -100],\n","       [-100,    0,    0, ..., -100, -100, -100],\n","       [-100,    7,    0, ..., -100, -100, -100]], dtype=int32)>)\n","---\n"]}]},{"cell_type":"markdown","metadata":{"id":"sIDGUmBIPfp4"},"source":["## Train our Token Classification Model\n","\n","We will now load the pretrained model and configure the required token labels for the model.\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"wOcCiXN-06lx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727278770987,"user_tz":-480,"elapsed":1355,"user":{"displayName":"NYP weech","userId":"03211871160483530494"}},"outputId":"48bfddd0-14c8-4b37-f2f0-5fab81a4dbc6"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForTokenClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n","- This IS expected if you are initializing TFDistilBertForTokenClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFDistilBertForTokenClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights or buffers of the TF 2.0 model TFDistilBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import TFAutoModelForTokenClassification, AutoConfig\n","\n","config = AutoConfig.from_pretrained(model_checkpoint, num_labels=len(label_names))\n","\n","model = TFAutoModelForTokenClassification.from_pretrained(\n","    model_checkpoint,\n","    config=config\n",")"]},{"cell_type":"markdown","metadata":{"id":"5YUbK5vVHM8m"},"source":["Let’s double-check that our model has the right number of labels:"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"GsS2UUWmHB_A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727278770988,"user_tz":-480,"elapsed":4,"user":{"displayName":"NYP weech","userId":"03211871160483530494"}},"outputId":"c4841595-08bd-42cf-cf7f-8aedbae43a6e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["9"]},"metadata":{},"execution_count":17}],"source":["model.config.num_labels"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"XTKNwOxZGDle","executionInfo":{"status":"ok","timestamp":1727278770988,"user_tz":-480,"elapsed":3,"user":{"displayName":"NYP weech","userId":"03211871160483530494"}}},"outputs":[],"source":["from transformers import create_optimizer\n","\n","num_epochs = 1\n","num_train_steps = len(train_dataset) * num_epochs\n","\n","optimizer, schedule = create_optimizer(\n","    init_lr=2e-5,\n","    num_warmup_steps=0,\n","    num_train_steps=num_train_steps,\n","    weight_decay_rate=0.01,\n",")\n","\n","# def dummy_loss(y_true, y_pred):\n","#     return tf.reduce_mean(y_pred)\n","\n","# losses = {\"loss\": dummy_loss}\n","# model.compile(loss=losses,optimizer=optimizer)\n","\n","model.compile(optimizer=optimizer)\n"]},{"cell_type":"markdown","metadata":{"id":"SbrA53lcH59z"},"source":["Huggingface model can actually compute loss internally — if you compile without a loss and supply your labels in the input dictionary (as we do in our datasets), then the model will train using that internal loss, which will be appropriate for the task and model type you have chosen.  This works previously using previous version of Tensorflow, but Tensorflow 2.7 seems to require a loss function to be supplied. So we create a dummy loss function as a work-around."]},{"cell_type":"code","execution_count":19,"metadata":{"id":"lJEGEiLiULBM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727279044298,"user_tz":-480,"elapsed":273312,"user":{"displayName":"NYP weech","userId":"03211871160483530494"}},"outputId":"0a4b2e1b-1e84-45e4-ce2a-2ece99c913da"},"outputs":[{"output_type":"stream","name":"stdout","text":["937/937 [==============================] - 246s 245ms/step - loss: 0.2271 - val_loss: 0.1145\n"]},{"output_type":"execute_result","data":{"text/plain":["<tf_keras.src.callbacks.History at 0x7fd4211ad630>"]},"metadata":{},"execution_count":19}],"source":["model.fit(\n","    train_dataset,\n","    validation_data=val_dataset,\n","    epochs=num_epochs\n",")"]},{"cell_type":"markdown","source":["## Evaluation\n","\n","The traditional framework used to evaluate token classification prediction is `seqeval`. To use this metric, we first need to install the `seqeval` library:"],"metadata":{"id":"mUZf2U9jZAq4"}},{"cell_type":"code","source":["!pip install seqeval\n","# !pip install datasets==1.2.1"],"metadata":{"id":"0QtA9FebYz5L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727279046934,"user_tz":-480,"elapsed":2649,"user":{"displayName":"NYP weech","userId":"03211871160483530494"}},"outputId":"8e227ef4-441b-4b54-c039-fa09ec20c44b"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.26.4)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.5.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n"]}]},{"cell_type":"code","execution_count":21,"metadata":{"id":"urzz3jP5LAcr","executionInfo":{"status":"ok","timestamp":1727279199598,"user_tz":-480,"elapsed":1104,"user":{"displayName":"NYP weech","userId":"03211871160483530494"}}},"outputs":[],"source":["from datasets import load_metric\n","\n","metric = load_metric('seqeval')"]},{"cell_type":"markdown","source":["In the following codes, we use our model to predict our val_dataset, in batches. For each batch of tf dataset, we have two parts: 1st contains the `input_ids`, `attention_masks`, and `labels`, while the second one is target label. We will only use the 1st part for prediction, i.e. `batch[0]`\n","\n","Also while looping through the list of predicted label for each token, we will ignore those positions that is labeled \"-100\"."],"metadata":{"id":"8jLt2o5EZZgC"}},{"cell_type":"code","execution_count":22,"metadata":{"id":"HSAWBeXZLAcr","colab":{"base_uri":"https://localhost:8080/","height":984},"executionInfo":{"status":"error","timestamp":1727279286671,"user_tz":-480,"elapsed":82644,"user":{"displayName":"NYP weech","userId":"03211871160483530494"}},"outputId":"372b777c-e454-47d7-b3f6-684a8f9fdfd1"},"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 0s 111ms/step\n","1/1 [==============================] - 0s 108ms/step\n","1/1 [==============================] - 0s 136ms/step\n","1/1 [==============================] - 0s 261ms/step\n","1/1 [==============================] - 0s 145ms/step\n","1/1 [==============================] - 0s 138ms/step\n","1/1 [==============================] - 0s 212ms/step\n","1/1 [==============================] - 0s 155ms/step\n","1/1 [==============================] - 0s 119ms/step\n","1/1 [==============================] - 0s 110ms/step\n","1/1 [==============================] - 0s 111ms/step\n","1/1 [==============================] - 0s 137ms/step\n","1/1 [==============================] - 0s 120ms/step\n","1/1 [==============================] - 0s 109ms/step\n","1/1 [==============================] - 0s 109ms/step\n","1/1 [==============================] - 0s 119ms/step\n","1/1 [==============================] - 0s 109ms/step\n","1/1 [==============================] - 0s 119ms/step\n","1/1 [==============================] - 0s 137ms/step\n","1/1 [==============================] - 0s 112ms/step\n","1/1 [==============================] - 0s 111ms/step\n","1/1 [==============================] - 0s 121ms/step\n","1/1 [==============================] - 0s 114ms/step\n","1/1 [==============================] - 0s 117ms/step\n","1/1 [==============================] - 0s 139ms/step\n","1/1 [==============================] - 0s 123ms/step\n","1/1 [==============================] - 0s 110ms/step\n","1/1 [==============================] - 0s 110ms/step\n","1/1 [==============================] - 0s 120ms/step\n","1/1 [==============================] - 0s 120ms/step\n","1/1 [==============================] - 0s 127ms/step\n","1/1 [==============================] - 0s 138ms/step\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-897f640e5a5f>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpredicted_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabel_idx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/tensor_getitem_override.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m    228\u001b[0m       \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_ops_stack\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m       packed_begin, packed_end, packed_strides = (\n\u001b[0;32m--> 230\u001b[0;31m           \u001b[0marray_ops_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m           \u001b[0marray_ops_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m           array_ops_stack.stack(strides))\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/array_ops_stack.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m       \u001b[0;31m# If the input is a constant list, it can be converted to a constant op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Input list contains non-constant tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m    711\u001b[0m   \u001b[0;31m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m   \u001b[0mpreferred_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreferred_dtype\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype_hint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m   return tensor_conversion_registry.convert(\n\u001b[0m\u001b[1;32m    714\u001b[0m       \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccepted_result_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_autopacking_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m   1292\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_autopacking_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m   \u001b[0;34m\"\"\"Tensor conversion function that automatically packs arguments.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mas_ref\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_should_not_autopack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1295\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m   \u001b[0minferred_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_dtype_from_nested_lists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_should_not_autopack\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m   1286\u001b[0m   \u001b[0;31m# pylint: disable=unidiomatic-typecheck\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m   \u001b[0;31m# TODO(slebedev): add nest.all?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_NON_AUTOPACKABLE_TYPES\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m   \u001b[0;31m# pylint: enable=unidiomatic-typecheck\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mflatten\u001b[0;34m(structure, expand_composites)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mnest\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msortable\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m   \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m   return nest_util.flatten(\n\u001b[0m\u001b[1;32m    294\u001b[0m       \u001b[0mnest_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCORE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36mflatten\u001b[0;34m(modality, structure, expand_composites)\u001b[0m\n\u001b[1;32m    710\u001b[0m   \"\"\"\n\u001b[1;32m    711\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodality\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCORE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tf_core_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mmodality\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tf_data_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36m_tf_core_flatten\u001b[0;34m(structure, expand_composites)\u001b[0m\n\u001b[1;32m    724\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m   \u001b[0mexpand_composites\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_pywrap_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import numpy as np\n","\n","all_predictions = []\n","all_labels = []\n","\n","for batch in val_dataset:\n","    logits = model.predict(batch[0])[\"logits\"]\n","    labels = batch[0][\"labels\"]\n","    predictions = np.argmax(logits, axis=-1)\n","    for prediction, label in zip(predictions, labels):\n","        for predicted_idx, label_idx in zip(prediction, label):\n","            if label_idx == -100:\n","                continue\n","            all_predictions.append(label_names[predicted_idx])\n","            all_labels.append(label_names[label_idx])\n","\n","metric.compute(predictions=[all_predictions], references=[all_labels])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BxwnjFtc4jZ2","executionInfo":{"status":"error","timestamp":1726452891602,"user_tz":-480,"elapsed":369,"user":{"displayName":"NYP weech","userId":"03211871160483530494"}},"colab":{"base_uri":"https://localhost:8080/","height":148},"outputId":"76ffe167-1701-47e5-fa51-a97a2d69d6dc"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'model' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-6db7c7040722>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'token_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}],"source":["model.save_pretrained('token_model')"]},{"cell_type":"markdown","source":["## Test on your sentence\n"],"metadata":{"id":"Cjzql4D_byUa"}},{"cell_type":"code","source":["!wget https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/pretrained-models/token_model.zip\n","!unzip token_model.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p4unn_5oqmtB","outputId":"fd3f7a0c-276f-4dfc-a3bd-d8f2307a7eca","executionInfo":{"status":"ok","timestamp":1727267619278,"user_tz":-480,"elapsed":15177,"user":{"displayName":"NYP weech","userId":"03211871160483530494"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-09-25 12:33:24--  https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/pretrained-models/token_model.zip\n","Resolving nyp-aicourse.s3.ap-southeast-1.amazonaws.com (nyp-aicourse.s3.ap-southeast-1.amazonaws.com)... 52.219.128.147, 3.5.151.144, 3.5.147.160, ...\n","Connecting to nyp-aicourse.s3.ap-southeast-1.amazonaws.com (nyp-aicourse.s3.ap-southeast-1.amazonaws.com)|52.219.128.147|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 244630549 (233M) [application/zip]\n","Saving to: ‘token_model.zip’\n","\n","token_model.zip     100%[===================>] 233.30M  23.7MB/s    in 11s     \n","\n","2024-09-25 12:33:36 (20.7 MB/s) - ‘token_model.zip’ saved [244630549/244630549]\n","\n","Archive:  token_model.zip\n","   creating: token_model/\n","  inflating: token_model/tf_model.h5  \n","  inflating: token_model/config.json  \n"]}]},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"obU1_ISe7fbJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def infer_tokens(text, model, tokenizer):\n","    # here we assume the text has not been splitted into individual words\n","    text = text.split()\n","\n","    encodings = tokenizer(\n","        [text],\n","        padding=True,\n","        truncation=True,\n","        is_split_into_words=True,\n","        return_tensors='tf')\n","\n","    logits = model(encodings)[0] # assume only a single prediction\n","    preds = np.argmax(logits, axis=-1)[0]\n","\n","    # as the prediction is on individual tokens, including subtokens,\n","    # we need to group subtokens belonging to the same word together\n","    # again, we use the word_ids to help us here\n","    previous_word_idx = None\n","    word_ids = encodings[0].word_ids\n","    labels = []\n","    for i, word_idx in enumerate(word_ids):\n","        # we check if the word_id different from previous one, then it is a new word\n","        # we also need to check if the word_id is not None so that we won't include it\n","        if word_idx != previous_word_idx and word_idx != None:\n","            labels.append(label_names[preds[i]])\n","        # update the previous_word_idx to current word_id\n","        previous_word_idx = word_idx\n","\n","    return text, labels"],"metadata":{"id":"Bb0SneGlb3f0"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mbR23gjFLAcs"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n","model = TFAutoModelForTokenClassification.from_pretrained('token_model')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G4ZMzkkNLAcv"},"outputs":[],"source":["sample_text = 'Ashish Vaswani has developed the transformer architecture during his time at Google.'\n","infer_tokens(sample_text, model, tokenizer)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":0}